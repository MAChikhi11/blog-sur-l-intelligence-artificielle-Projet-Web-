<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="../../css/nav.css">
    <link rel="stylesheet" href="../../css/article.css">
    <link rel="stylesheet" href="../../css/footer.css">
    <style>
        img{ border-radius: 8px;}
        section h3 { color: #919efe;}
        .date {color: #7785f1; font-weight: bold; font-size: 26px;}
        .quot {color: #919efe;}
        section h2 { color: blueviolet; }
        section h4 {text-align: left;
            font-size: 25px;
    color: blueviolet;}
    </style>
    <title>Histoire</title>
    <link rel="shortcut icon" type="image/png" href="../../images/icon.png"/>

</head>
<body>
    <header>
        <nav>
         <div class="logo">
             
             <img src="../../images/logo.jpg" alt="" class="logo"> 
         </div>  
         
         <ul class="nav-list">
             <li class="list-item"> <a href="../../index.html">Accueil</a> </li>
             <li class="list-item"> <a href="../Sommaire.html" style="font-weight: bold; color: #919efe;">Articles</a> </li>
             <li class="list-item"> <a href="../Apropos.html">A propos</a> </li>
         </ul>
        </nav>
    </header> 
    <main>
        <div class="container">
            <!-- un sidebar qui permet de naviguer entre les articles -->
            <aside style="border-left: solid 3px #919efe;">
                <h3>Articles</h3>
                <ul>
                    <li><a href="Intro.html">Introduction</a></li>
                    <li><a href="Histoire.html" style="color:#919efe; font-weight: bold; ">Histoire</a></li>
                    <li><a href="Domaines.html">Domaines</a></li>
                    <li><a href="Applications.html" >Applications</a></li>
                    <li><a href="Future.html">Future</a></li>
                </ul>
            </aside>
            <Section>
                <div class="title">
                    <h2 style="color: #919efe;">Histoire de  l'intelligence artificielle  </h2>
                    <div>Apr 05, 2022 · 19 min read</div>
                </div>
                <img src="../../images/Histoire/Evolution_IA.png " style="height: 100%;width: 100%;" alt="Image évolution IA">
                <div>
                    <p>L’intelligence artificielle (IA) est une discipline jeune d’une soixante d’années, qui est un ensemble de sciences, théories et techniques (notamment logique mathématique, statistiques, probabilités, neurobiologie computationnelle, informatique) qui ambitionne d’imiter les capacités cognitives d’un être humain. Initiés dans le souffle de la seconde guerre mondiale, ses développements sont intimement liés à ceux de l’informatique et ont conduit les ordinateurs à réaliser des tâches de plus en plus complexes, qui ne pouvaient être auparavant que déléguées à un humain.</p>
                    <p>Cette automatisation demeure toutefois loin d’une intelligence humaine au sens strict, ce qui rend la dénomination critiquable pour certains experts. Le stade ultime de leurs recherches (une IA « forte », c’est-à-dire en capacité de contextualiser des problèmes spécialisés très différents de manière totalement autonome) n’est absolument pas comparable aux réalisations actuelles (des IA « faibles » ou « modérées », extrêmement performantes dans leur domaine d’entraînement). L’IA « forte », qui ne s’est encore matérialisée qu’en science-fiction, nécessiterait des progrès en recherche fondamentale (et non de simples améliorations de performance) pour être en capacité de modéliser le monde dans son ensemble.</p>
                    <p>
                        Depuis 2010, la discipline connaît toutefois un nouvel essor du fait, principalement, de l’amélioration considérable de la puissance de calcul des ordinateurs et d’un accès à des quantités massives de données.
                    </p>
                    <p>Les promesses, renouvelées, et les inquiétudes, parfois fantasmées, complexifient une compréhension objective du phénomène. De brefs rappels historiques peuvent contribuer à situer la discipline et éclairer les débats actuels.</p>

                </div> <!-- Fin section Introduction  -->
                <h3>Plus loin dans l'histoire</h3>
                <img src="../../images/Histoire/AI_machine_a_ecrire.jpg " style="height: 100%;" alt="National Museum of Computing">
                
                <ul>
                    Le terme “Intelligence artificielle” serait apparut que très recemment. Mais le concept d’une machine ou objet capable de raisonnement et de pensée complexe remonte bien plus loin dans l’Histoire de l’humanité. C’est notamment à travers les anciens mythes que l’on trouve le plus de récits illustrant cette notion, comme avec : 
                    <li>Le mythe de Talos : un automate géant fait de bronze ayant pour but de protéger Europe, mère du Roi de Crète. </li>
                    <li>La statue de Pygmalion qui prend vie.</li>
                    <li>Dans la mythologie juive, avec le Golem, humanoïde fait d’argile qui se voit insuffler la vie afin de défendre son créateur. </li>
                </ul>
                <figure>
                    <img src="../../images/Histoire/Talos.jpg " width="300" height="400" alt="Image Talos">
                    <figcaption>Talos,automate de bronze qui garde l'île de Crète en lancant des blocs de pierres sur les navires </figcaption>
                </figure>
                <p>
                    C’est à croire que l’idée de créer la vie et élaborer une copie de nous même est profondément ancrée dans la psyché humaine. <br>
                    Mais les philosophes de nombreuses civilisations se sont intéressés à l’idée de comprendre la pensée humaine par une approche mécanisée, ce qui implique la possibilité de la reproduire. C’est le début de la logique et du raisonnement. Au 4e siècle av. J.-C., Aristote (384 av. J.-C - 322 av. J.-C.) invente un système de raisonnement logique appelé <em>“Syllogisme”</em>. C’est important, car la programmation informatique est un prolongement de l’approche mathématique de la logique. <br>
                    Le syllogisme fonctionne en mettant en relation trois propositions afin d’en déduire un résultat. Un exemple très connu :
                    </p>
                    <blockquote class="quot" >
                        “Tous les hommes sont mortels, or Socrate est un homme donc Socrate est mortel”.
                    </blockquote> 
                    <p>
                        <span class="date">Au 17e siècle,</span>  plusieurs philosophes explorent la possibilité que la pensée puisse être expliqué par un ensemble d'opérations algorithmiques ce qui fait de l'être humain et les animaux, ni plus ni moins que des machines complexes. C’est le cas avec Leibniz (1646 - 1716) qui a spéculé que la raison humaine pourrait être réduite à un calcul mécanique et à qui on doit la première utilisation d’un système binaire. Mais aussi Renée Descartes (1596 - 1650) et Hobbes (1588 - 1679) avec sa célèbre citation : “reason is nothing but reckoning” que se traduit par : “La raison n’est rien d’autre que du calcul mathématique”.
                        </p>
                    <p>
                            <span class="date">Au 18e siècle,</span>  les jouets mécaniques sont très populaires. En 1769, l'ingénieur hongrois Baron Wolfgang von Kempelen (1734 - 1804) construisit une machine à jouer aux échecs pour amuser la reine autrichienne Maria Theresia. C'était un appareil mécanique, appelé le Turc (car ressemblant à un homme turc traditionnel de l'époque). Un des automates les plus célèbres de l'histoire, mais qui étaient en réalité un canular bien ficelé.  <br>
                            Toutefois, cela montre la fascination de construire des machines capables de s'adonner à des activités intellectuelles comme les échecs. 
                    </p>
                    <figure>
                        <img src="../../images/Histoire/Turc_mecanique.jpg" alt="Image du turc mécanique" width="500" height="300">
                        <figcaption>Le turc mécanique, l'automate qui jouait au échecs</figcaption>
                    </figure>
                    <p>    
                    <span class="date">Au 19e siècle,</span>  en 1818, Marry Sheley (1797 - 1851) publie l’une des premières histoires de science-fiction, “Le monstre de Frankenstein”, qui raconte l’histoire d’une créature conçue de toute pièce. Cette dernière prend vie et devient folle, se rebellant contre son créateur. Une thématique reprise de nombreuses fois par la suite, surtout dans le domaine de l’intelligence artificielle, robots et machines.
                    </p>
                    <figure>
                        <img src="../../images/Histoire/monstre_frankenstein.jpg" alt="Image du livre le Monstre de Frankenstein">
                        <figcaption>Le monstre de Frankenstein, livre de Marry Shelley</figcaption>
                    </figure>
                    <p>
                        <span class="date">Au 20e siècle,</span> l'étude de la logique mathématique a fourni la percée essentielle qui a rendu l'intelligence artificielle plausible. Les acteurs majeurs étant Bertrand Russell et Alfred North Whitehead avec l’ouvrage “Principia Mathematica” en 1913. La première machine fonctionnelle capable de jouer aux échecs remonte à 1912 avec l’invention de “Ajedrecista” par Leonardo Torres. 
                    </p>
                    <p>Le terme “robot” est utilisé pour la première fois dans une pièce de théâtre à Londres en 1923, écrit par Karel Capek (1890 - 1938). Alors bien sûr, l’histoire de l’intelligence artificielle côtoie étroitement celle de l’informatique. Le 20e siècle peut donc surement être considéré comme le siècle qui a vu naître cette discipline.</p>
                    <p> <span class="date">En 1936,</span> Alan Turing et Alonzo Church publient la thèse de Church-Turing, une hypothèse sur la nature des dispositifs de calcul mécanique. La thèse soutient que tout calcul possible peut être effectué par un algorithme exécuté sur un ordinateur, à condition que suffisamment de temps et d'espace de stockage soient disponibles. Les premiers ordinateurs modernes furent les machines destinées à déchiffrer des codes nazis durant la Seconde Guerre mondiale (ENIAC et Colossus). Un des premiers ordinateurs programmables est attribué à l’ingénieur allemand Konrad Zuse (1910 - 1995) en 1941 avec le Z3.</p>
                    
                    <figure>
                        <img src="../../images/Histoire/Z3_computer.jpg" alt="Image du Z3 computer" width="600" height="400">
                        <figcaption>Le Z3, un des premiers ordinateurs programmables, avec son créateur Konrad Zuse</figcaption>
                    </figure>
                    <p>Le concept de machine pensante se répand dans les années 40 et 50 avec des personnalités comme John Von Neumann (1903 - 1957), Norbert Wiener (1894 - 1964) et Claude Shannon (1916 - 2001).</p>
                    <h4>Table des dates clés </h4>
                    <table>
                        <tr>
                          <th>Date</th>
                          <th>Evenement</th>
                          
                        </tr>
                        <tr>
                            <td class="d">1689</td>
                            <td>invention du système binaire par Gottfried Leibniz </td>
                            
                          </tr>
                          <tr>
                            <td class="d">1769 </td>
                            <td>Construction du turc, un des automates les plus célèbres de l’histoire  </td>
                            
                          </tr>
                          <tr>
                            <td class="d">1818  </td>
                            <td>publication de l’une des premières œuvres de sciences fiction <br> sur une créature intelligente "Le monstre de Frankenstein  </td>
                            
                          </tr>
                          
                        <tr>
                          <td class="d">1923</td>
                          <td>Première apparition du terme robot </td>
                          
                        </tr>
                        <tr>
                          <td class="d">1936</td>
                          <td>Concept du premier ordinateur moderne</td>
                          
                        </tr>
                        <tr>
                          <td class="d">1950</td>
                          <td>Le test de Turing est imaginé</td>
                          
                        </tr>
                        <tr>
                          <td class="d">1956</td>
                          <td>Première apparition du terme intelligence artificielle </td>
                          
                        </tr>
                        <tr>
                          <td class="d">1956</td>
                          <td>Yoshi Tannamuri</td>
                        </tr>
                      </table>
                    <h3>Du 20ème siècle à nos jours ...</h3>
                    <h2 style="margin: 40px 0;">1940-1960 : Naissance de l’IA dans le sillage de la cybernétique</h2>
                    <p>L’époque entre 1940 et 1960 a été fortement marquée par la conjonction de développements technologiques (dont la seconde guerre mondiale a été un accélérateur) et la volonté de comprendre comment faire se rejoindre le fonctionnement des machines et des êtres organiques. Ainsi pour Norbert Wiener, pionnier de la cybernétique, l’objectif était d’unifier la théorie mathématique, l’électronique et l’automatisation en tant que « théorie entière de la commande et de la communication, aussi bien chez l’animal que dans la machine ». Juste auparavant, un premier modèle mathématique et informatique du neurone biologique (neurone formel) avait été mis au point par Warren McCulloch et Walter Pitts dès 1943.</p>
                    <p>Début 1950, John Von Neumann et Alan Turing ne vont pas créer le terme d’IA mais vont être les pères fondateurs de la technologie qui la sous-tend : ils ont opéré la transition entre les calculateurs à la logique décimale du XIXème siècle (qui traitaient donc des valeurs de 0 à 9) et des machines à la logique binaire (qui s’appuient sur l’algèbre booléenne, traitant des chaines plus ou moins importantes de 0 ou de 1). Les deux chercheurs ont ainsi formalisé l’architecture de nos ordinateurs contemporains et ont démontré qu’il s’agissait là d’une machine universelle, capable d’exécuter ce qu’on lui programme. Turing posera bien en revanche pour la première fois la question de l’éventuelle intelligence d’une machine dans son célèbre article de 1950 « Computing Machinery and Intelligence » et a décrit un « jeu de l’imitation », où un humain devrait arriver à distinguer lors d’un dialogue par téléscripteur s’il converse avec un homme ou une machine. Pour polémique que soit cet article (ce « test de Turing » n’apparaît pas qualifiant pour nombre d’experts), il sera souvent cité comme étant à la source du questionnement de la limite entre l’humain et la machine.</p>
                    <figure>
                        <img src="../../images/Histoire/Computer_machinery_and_intelligence.jpeg" alt="Computer machinery and intelligence by Alan Turing">
                        <figcaption>Computer machinery and intelligence - Article de Alan Turing</figcaption>
                    </figure>
                    <p> La paternité du terme « IA » pourrait être attribué à John McCarthy du MIT (Massachusetts Institute of Technology), terme que Marvin Minsky (université de Carnegie-Mellon) définit comme « la construction de programmes informatiques qui s’adonnent à des tâches qui sont, pour l’instant, accomplies de façon plus satisfaisante par des êtres humains car elles demandent des processus mentaux de haut niveau tels que : l’apprentissage perceptuel, l’organisation de la mémoire et le raisonnement critique ». La conférence durant l’été 1956 au Dartmouth College (financée par le Rockefeller Institute) est considérée comme fondatrice de la discipline. De manière anecdotique, il convient de relever le grand succès d’estime de ce qui n’était pas une conférence mais plutôt un atelier de travail. Seulement six personnes, dont McCarthy et Minsky, étaient restées présentes de manière constante tout au long de ces travaux (qui s’appuyaient essentiellement sur des développements basés sur de la logique formelle).</p>
                    <p>Si la technologie demeurait fascinante et remplie de promesse (voir notamment dans le domaine judiciaire l’article de Reed C.Lawlor, avocat au barreau de Californie, de 1963 « What Computers Can Do : Analysis and Prediction of Judicial Decisions »), l’engouement est retombé au début des années 1960. Les machines disposaient en effet de très peu de mémoire, rendant malaisé l’utilisation d’un langage informatique. On y retrouvait toutefois déjà certains fondements encore présents aujourd’hui comme les arbres de recherche de solution pour résoudre des problèmes : l’IPL, information processing language, avait permis ainsi d’écrire dès 1956 le programme LTM (logic theorist machine) qui visait à démontrer des théorèmes mathématiques.</p>
                    <p>Herbert Simon, économiste et sociologue, a eu beau prophétiser en 1957 que l’IA arriverait à battre un humain aux échecs dans les 10 années qui suivraient, l’IA est entrée alors dans un premier hiver. La vision de Simon s’avérera pourtant juste… 30 années plus tard.</p>
                    <h2 style="margin: 40px 0;">1980-1990 : Les systèmes experts</h2>
                    <p>En 1968 Stanley Kubrick réalisera le film « 2001 l’Odyssée de l’espace » où un ordinateur – HAL 9000 (distant que d’une seule lettre de celles d’IBM) résume en lui-même toute la somme de questions éthiques posées par l’IA : arrivée à un haut niveau de sophistication, celle-ci représentera-t-elle un bien pour l’humanité ou un danger ? L’impact du film ne sera naturellement pas scientifique mais il contribuera à vulgariser le thème, tout comme l’auteur de science-fiction Philip K. Dick, qui ne cessera de s’interroger si, un jour, les machines éprouveront des émotions.</p>
                    <p>C’est avec l’avènement des premiers microprocesseurs fin 1970 que l’IA reprend un nouvel essor et entre dans l’âge d’or des systèmes experts.</p>
                    <p> La voie avait été en réalité ouverte au MIT dès 1965 avec DENDRAL (système expert spécialisé dans la chimie moléculaire) et à l’université de Stanford en 1972 avec MYCIN (système spécialisé dans le diagnostic des maladies du sang et la prescription de médicaments). Ces systèmes s’appuyaient sur un « moteur d’inférence », qui était programmé pour être un miroir logique d’un raisonnement humain. En entrant des données, le moteur fournissait ainsi des réponses d’un haut niveau d’expertise.</p>
                    <p>Les promesses laissaient envisager un développement massif mais l’engouement retombera à nouveau fin 1980, début 1990. La programmation de telles connaissances demandait en réalité beaucoup d’efforts et à partir de 200 à 300 règles, il y avait un effet « boîte noire » où l’on ne savait plus bien comment la machine raisonnait. La mise au point et la maintenance devenaient ainsi extrêmement problématiques et – surtout – on arrivait à faire plus vite et aussi bien d’autres manières moins complexes, moins chères. Il faut rappeler que dans les années 1990, le terme d’intelligence artificielle était presque devenu tabou et des déclinaisons plus pudiques étaient même entrées dans le langage universitaire, comme « informatique avancée ».</p>
                    <p>Le succès en mai 1997 de Deep Blue (système expert d’IBM) au jeu d’échec contre Garry Kasparov concrétisera 30 ans plus tard la prophétie de 1957 d’Herbert Simon mais ne permettra pas de soutenir les financements et les développements de cette forme d’IA. Le fonctionnement de Deep Blue s’appuyait en effet sur un algorithme systématique de force brute, où tous les coups envisageables étaient évalués et pondérés. La défaite de l’humain est restée très symbolique dans l’histoire mais Deep Blue n’était en réalité parvenu à ne traiter qu’un périmètre très limité (celui des règles du jeu d’échec), très loin de la capacité à modéliser la complexité du monde.</p>
                    <figure>
                        <img src="../../images/Histoire/Kasparov.jpeg" alt="Le champion du monde d'échecs kasparov face a Deep Blue" width="500" height="300">
                        <figcaption>Garry Kasparov face à Deep Blue - 1997</figcaption>
                    </figure>
                    <h2 style="margin: 40px 0;">Depuis 2010 : un nouvel essor à partir des données massives et d’une nouvelle puissance de calcul</h2>
                    <ul>
                        Deux facteurs expliquent le nouvel essor de la discipline aux alentours de 2010 :
                        <li>L’accès tout d’abord à des volumes massifs des données. Pour pouvoir utiliser des algorithmes de classification d’image et de reconnaissance d’un chat par exemple, il fallait auparavant réaliser soi-même un échantillonnage. Aujourd’hui, une simple recherche sur Google permet d’en trouver des millions.</li>
                        <li>Ensuite la découverte de la très grande efficacité des processeurs de cartes graphiques des ordinateurs pour accélérer le calcul des algorithmes d’apprentissage. Le processus étant très itératif, cela pouvait prendre des semaines avant 2010 pour traiter l’intégralité d’un échantillonnage. La puissance de calcul de ces cartes, (capables de plus de mille milliards d’opérations par seconde) a permis un progrès considérable pour un coût financier restreint (moins de 1000 euros la carte).</li>
                    </ul>
                    <figure>
                        <img src="../../images/Histoire/puissance_calcul.jpg" alt="Image evolution puissance calcul" width="500" height="500">
                        <figcaption>Courbe de la croissance exponentielle de la puissance de calcul vu par Ray Kurzweil dans "The singularity is Near" </figcaption>
                    </figure>


                    <p>
                    Ce nouvel attirail technologique a permis quelques succès publics significatifs et a relancé les financements : en 2011, Watson, l’IA d’IBM, remportera les parties contre 2 champions du « Jeopardy ! ». En 2012, Google X (laboratoire de recherche de Google) arrivera à faire reconnaître à une IA des chats sur une vidéo. Plus de 16 000 processeurs ont été utilisés pour cette dernière tâche, mais le potentiel est alors extraordinaire : une machine arrive à apprendre à distinguer quelque chose. En 2016, AlphaGO (IA de Google spécialisée dans le jeu de Go) battra le champion d’Europe (Fan Hui) et le champion du monde (Lee Sedol) puis elle-même (AlphaGo Zero). Précisons que le jeu de Go a une combinatoire bien plus importante que les échecs (plus que le nombre de particules dans l’univers) et qu’il n’est pas possible d’avoir des résultats aussi significatifs en force brute (comme pour Deep Blue en 1997).
                    </p>
                    <figure>
                        <img src="../../images/Histoire/alphago-lee-sedol.jpg" alt="Le champion du monde du jeu G face a AlphaZero" width="500" height="300">
                        <figcaption>Lee Sedol face à AlphaZero - 2016</figcaption>
                    </figure>
                    <p>D’où vient ce miracle ? D’un changement complet de paradigme par rapport aux systèmes experts. L’approche est devenue inductive : il ne s’agit plus de coder les règles comme pour les systèmes experts, mais de laisser les ordinateurs les découvrir seuls par corrélation et classification, sur la base d’une quantité massive de données.</p>
                    <p>Parmi les techniques d’apprentissage machine (machine learning), c’est celle de l’apprentissage profond (deep learning) qui paraît la plus prometteuse pour un certain nombre d’application (dont la reconnaissance de voix ou d’images). Dès 2003, Geoffrey Hinton (de l’Université de Toronto), Yoshua Bengio (de l’Université de Montréal) et Yann LeCun (de l’Université de New York) avaient décidé de démarrer un programme de recherche pour remettre au goût du jour les réseaux neuronaux. Des expériences menées simultanément à Microsoft, Google et IBM avec l’aide du laboratoire de Toronto de Hinton ont alors démontré que ce type d’apprentissage parvenait à diminuer de moitié les taux d’erreurs pour la reconnaissance vocale. Des résultats similaires ont été atteints par l’équipe de Hinton pour la reconnaissance d’image.</p>
                    <p>Du jour au lendemain, une grande majorité des équipes de recherche se sont tournées vers cette technologie aux apports incontestables. Ce type d’apprentissage a aussi permis des progrès considérables pour la reconnaissance de texte, mais, d’après les experts comme Yann LeCun, il y a encore beaucoup de chemin à parcourir pour produire des systèmes de compréhension de texte. Les agents conversationnels illustrent bien ce défi : nos smartphones savent déjà retranscrire une instruction mais ne parviennent pas la contextualiser pleinement et à analyser nos intentions.</p>
                    <h3>Quelques dates clés </h3>
                        <img src="../../images/Histoire/history.jpg" style="height:100%" >
                    <!-- un bouton qui permet de remonter au debut de l'article -->
                    <div class="top">
                       <img src="../../images/top.png" alt="">
                       <a href="Histoire.html" style="color: #919efe;">BACK TO TOP</a>
                    </div> 
                   
            </Section>
        </div>
    </main>
    <footer>Ai blog Copyright © 2022 . Tous les droits reserves.</footer>
</body>
</html>